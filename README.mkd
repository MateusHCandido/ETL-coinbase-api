# Data Pipeline: Extrção de Dados Bitcoin com ETL em Python

 O objetivo dessa pipilina é extrair dados de uma API(Coibase), organizar esses dados e armazenar em uma base de dados utilizando o processo ETL através do Apache Airflow.

## Tecnologia utilizadas
- `Python` 4.7.3
- `Docker`
- Bibliotecas: 
    - `datetime`: Para manipulação de datas e horas.
- Frameworks:
    - `airflow`: Orquestrador de pipelines
        - apache-airflow-providers-postgres
        - apache-airflow-providers-http

Obs: Para essa versão do projeto, é necessário a utilização do docker para poder subir as instâncias do aiflow e seu servidor web.



# Primeira Fase do projeto - Criação da DAG

 A DAG será um módulo com uma configuração determinada para objetivar sua função no airflow. Dentro da DAG serão distribuídas tarefas, em que cada tarefa consiste em uma etapa no pipeline da dados.


 ```
    dag = DAG('extract_data_coinbase_api', #DAG name
          'extraction of data by Coinbase API', #Description
          start_date=datetime(2024, 12, 29),
          schedule_interval=timedelta(seconds=15),  
          catchup=False) #Avoid retroactive execution
 ```

# Segunda Fase do projeto - Criação das tarefas

## Tarefa 1 - extract_btc_data_task

 Efetua a extração dos dados da API Coinbase

```
    extract_btc_data_task = SimpleHttpOperator(
    task_id='extract_btc_data',
    http_conn_id='coinbase_api', #Connection configured in airflow
    endpoint='v2/prices/spot',
    method='GET',
    response_filter=lambda response: response.json()['data'],
    log_response=True,
    dag=dag
    )
 ```


## Tarefa 2 - transform_btc_data_task

 Uma tarefa com um operador Python em que permite dentro do airflow, realizar codificação Python. No caso da aplicação, foi utilizado para replicar a função transform_btc_data() e refatora-la para se adaptar ao uso do airflow

 - Função transform_btc_data

 ```
 def transform_btc_data(ti):
    json_data = ti.xcom_pull(task_ids='extract_btc_data')
    if json_data:
        valor = json_data['amount']
        criptomoeda = json_data['base']
        moeda = json_data['currency']

        transformed_value = {
            'valor': valor,
            'criptomoeda': criptomoeda,
            'moeda': moeda,
            'data_hora': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        return transformed_value
    raise ValueError('Not found data in xcom for "extract_btc_data" method.')
 ```

 - Função transform_btc_data_task

 ```
    transform_btc_data_task = PythonOperator(
    task_id='transform_btc_data',
    python_callable=transform_btc_data,
    provide_context=True,
    dag=dag
)
 ```

## Tarefa 3 - load_btc_data_task

Tarefa responsável por carregar os dados dentro do banco.
 Ela também atua como um operador python, e nesse contexto, a função load_btc_data quem utiliza os módulos auxiliares de criação de tabela, caso não exista, e inserção dos dados transformados

 - load_btc_data

 ```
    def load_btc_data(ti):
        data = ti.xcom_pull(task_ids='transform_btc_data')
        if not data:
            raise ValueError('Not found data in xcom for "load_btc_data" method.')
        else:
            try:
                create_table()
                insert_data(data)
            except Exception as e:
                print(f'An error occurred when trying to load the data:: {e}')

 ```

 Obs: as funções [create_table()](/dags/modules/create_table.py) e [insert_data(data)](/dags//modules/insert_table.py) foram modularizadas, afim de trazer clareza e organização para o código
 - load_btc_data_task

 ```
    load_btc_data_task = PythonOperator(
    task_id='load_btc_data',
    python_callable=load_btc_data,
    dag=dag
)
 ```

# Segunda fase do projeto - Configuração de conexões no airflow

 Siga os seguintes passos:

 - Ir na área Admin>Connections.

 - Criar conexão com a API Coinbase
    - `Conn Id` = coinbase_api
    - `Conn Type` = http
    - `Host` = https://api.coinbase.com

 - Criar conexão com o Postgres
    - `Conn Id` = postgres
    - `Conn Type` = postgres
    - `Host` = postgres
    - `Port` = 5432

Obs: essas configurações estão seguindo as configurações feitas no módulo [pipeline.py](/dags/pipeline.py), que acompanha as configurações feitas no [docker-compose.yml](/docker-compose.yaml), então para não ter problemas na execução, executar nessa ordem

# Terceira fase - Definição de precedência da execução

 Dentro do módulo onde foi feita DAG, deve-se definir essa ordem de precedência. 
    
```
extract_btc_data_task >> transform_btc_data_task >> load_btc_data_task
```

 O `>>` representa uma sequência lógica e também de padrão em que só será passado para uma nova tarefa, caso a anterior já tenha sido bem sucedida


## Execute o projeto

### Observação:

 Até o momento atual (28/12/2024), a API do Coinbase está estável e funcional de forma gratuita. Para testar, pode colar o endpoint(`https://api.coinbase.com/v2/prices/spot`) em seu navegador. O será semelhante a isso:

 ```
// 20241228211236
// https://api.coinbase.com/v2/prices/spot

{
  "data": {
    "amount": "95033.375",
    "base": "BTC",
    "currency": "USD"
  }
}
 ```

## Do que preciso para rodar esse projeto?

- Para esse projeto, é necessária as seguintes ferramentas:

    - Python 4.7.3
    - Docker


- Quando copiado o projeto, verifique em sua máquina se a porta 5432 e 8080 estão disponíveis.

- No diretório raiz, onde contém o arquivo docker-compose.yml, execute o comando `docker-compose up -d`, ele irá subir as instâncias dos airflow e do postgres.
    - Garanta que o postgres será executado, caso não o servidor web do airflow não será executado.

- Quando executado, acesse o `http://localhost:8080/` para abrir o airflow
    - Login = `airflow`
    - Senha = `airflow`

- Assim que entrar, será apresentada apenas uma DAG pausada. Você pode ativa-la e executa-la manualmente para que possa acompanhar o pipeline.
     - Mantenha caultela pois a DAG está programada para ser executada a cada 15s, então os dados da sua aplicação serão consumidos e armazenados no banco nesse período. Assim que terminar de usar, deve-se pausar a dag novamente ou desligar os serviços através do `docker-compose down`